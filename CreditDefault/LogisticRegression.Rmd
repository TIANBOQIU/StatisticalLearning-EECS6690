---
title: "LogisticRegression"
author: "Tianbo Qiu"
date: "December 15, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

read the data
```{r}
data = read.csv("CreditDataRaw.csv",encoding="UTF-8")
names(data)[1] <- "ID"
names(data)[25] <- "Payment" #default.payment.next.month
High = ifelse(data$Payment==0,"NOT","DEFAULT")
data = data.frame(data,High)
set.seed(10)
DATASIZE = dim(data)[1]
train_index = sample(DATASIZE, DATASIZE*0.8)
indicator = rep(0,DATASIZE)
indicator[train_index] = 1 # 1:train 0:valid
data = data.frame(data,indicator)
train = subset(data,data$indicator==1)
valid = subset(data,data$indicator==0)
```

```{r}
names = names(train)
f <- as.formula(paste("Payment ~", paste(names[!names %in% c("Payment","ID", "indicator","High")], collapse = " + ")))
```



# norm the data
```{r}
# Scale the Feature into the [0,1] interval
scale01 <- function(x){
  (x - min(x)) / (max(x) - min(x))
}
train_norm = train
valid_norm = valid
for(name in names(train)){
  if(name != "cl" && name != "ID" && name !="Payment" && name != "High"  && name != "indicator"){
    train_norm[name] <- scale01(train_norm[name])
    valid_norm[name] <- scale01(valid_norm[name])
  }
  
}

```

Logistic Regression
```{r}
glm_fit = glm(f, data=train_norm, family = binomial)
summary(glm_fit)
```
```{r}
# after we checked p-value of each feature
names = names(train)
f2 <- as.formula(paste("Payment ~", paste(names[!names %in% c("Payment","ID", "indicator","High", "PAY_AMT6", "PAY_AMT5", "PAY_AMT4", "PAY_AMT3", "BILL_AMT6", "BILL_AMT5", "BILL_AMT4", "BILL_AMT3", "BILL_AMT2", "PAY_6", "PAY_5", "PAY_4")], collapse = " + ")))
fit2 = glm(f2, data=train_norm, family = binomial)
summary(fit2)
```


gains
```{r}
# training
pred = predict(fit2, train_norm, type = "response")
PredLabel = data.frame(round(pred))
names(PredLabel) <- "PredLabel"
PredScore = data.frame(pred)
names(PredScore) <- "PredScore"
train_norm = data.frame(train_norm,PredLabel, PredScore)
tt = table(pred = train_norm$PredLabel, actual = train_norm$Payment)
error_train = 1 - sum(diag(tt)) / sum(tt)

# validation
predV = predict(fit2, valid_norm, type = "response")
PredLabelV = data.frame(round(predV))
names(PredLabelV) <- "PredLabelV"
PredScoreV = data.frame(predV)
names(PredScoreV) <- "PredScoreV"
valid_norm = data.frame(valid_norm,PredLabelV, PredScoreV)
tv = table(pred = valid_norm$PredLabelV, actual = valid_norm$Payment)
error_valid = 1 - sum(diag(tv)) / sum(tv)

error_train
error_valid
```


```{r}
library(gains)
library(rlang)
gtt = gains(actual=train_norm$Payment, predicted=train_norm$PredScore,optimal=TRUE)
cpt_y = gtt$cume.pct.of.total
#cpt_y = prepend(cpt_y,0,before=1)
cpt_x = gtt$depth
#cpt_x = prepend(cpt_x,0,before=1)
#fit = lm(cpt_y~poly(cpt_x,3,raw=TRUE))
predicted = table(train_norm$PredLabel)[2]
xx = cpt_x / 100 * 24000
yy = cpt_y * predicted
#plot(xx,yy)
xx = prepend(xx,0,before=1)
yy = prepend(yy,0,before=1)
fit = lm(yy~poly(xx,3,raw=TRUE))
xx = 0:24000
model_yy = predict(fit,data.frame(xx))

png("Logistic_Regression_lift_chart_train.png")
plot(xx, model_yy, col="green",xlab="Number of total data", ylab="Cumulative number of target data")
best_yy = rep(predicted,24001)
for(i in 0:predicted){
  best_yy[i+1] = i
}
lines(xx,best_yy,col="red",lwd=3)
base_yy = predicted / 24000 * xx
lines(xx,base_yy,col="blue")
legend(17500,750, legend=c("best curve","model", "baseline"), col = c("red","green","blue"), lwd=c(1,1,1),cex=1)
title("Lift chart of Logistic Regression (training)")
dev.off()
library(geiger)
#area1 = geiger:::.area.between.curves(xx,base_yy,yy)
#area2 = geiger:::.area.between.curves(xx,best_yy,yy)
#area1 / area2
a1 = sum(model_yy-base_yy)
a2 = sum(best_yy-base_yy)
a1/a2
```
gains for validation

```{r}
library(gains)
library(rlang)
gtv = gains(actual=valid_norm$Payment, predicted=valid_norm$PredScoreV,optimal=TRUE)
cpv_y = gtv$cume.pct.of.total
#cpt_y = prepend(cpt_y,0,before=1)
cpv_x = gtv$depth
#cpt_x = prepend(cpt_x,0,before=1)
#fit = lm(cpt_y~poly(cpt_x,3,raw=TRUE))
predictedValid = table(valid_norm$PredLabelV)[2]
xxv = cpv_x / 100 * 6000
yyv = cpv_y * predictedValid
#plot(xx,yy)
xxv = prepend(xxv,0,before=1)
yyv = prepend(yyv,0,before=1)
fit = lm(yyv~poly(xxv,3,raw=TRUE))
xxv = 0:6000
model_yyv = predict(fit,data.frame(xxv))

png("Logistic_Regression_lift_chart_validation.png")
plot(xxv, model_yyv, col="green",xlab="Number of total data", ylab="Cumulative number of target data")
best_yyv = rep(predictedValid,6001)
for(i in 0:predictedValid){
  best_yyv[i+1] = i
}
lines(xxv,best_yyv,col="red",lwd=3)
base_yyv = predictedValid / 6000 * xxv
lines(xxv,base_yyv,col="blue")
legend(17500,750, legend=c("best curve","model", "baseline"), col = c("red","green","blue"), lwd=c(1,1,1),cex=1)
title("Lift chart of Logistic Regression (validation)")
dev.off()
library(geiger)
#area1 = geiger:::.area.between.curves(xx,base_yy,yy)
#area2 = geiger:::.area.between.curves(xx,best_yy,yy)
#area1 / area2
a1 = sum(model_yyv-base_yyv)
a2 = sum(best_yyv-base_yyv)
a1/a2
```

Use the SSM(Sorting Smoothing Method) to estimate the real probability
```{r}
#1. order the valid data according to predictive probability
valid_norm_sort = valid_norm[order(PredKNNScoreV),]
#2. use SSM formula to evaluate actural probability 'Pi', we choose n =50 according to the paper
VALIDSIZE = dim(valid_norm)[1]
n = 50
actural_p_valid = rep(0,VALIDSIZE)
#pred_valid = valid_sort$PredTreeScoreValid
pred_valid = round(valid_norm_sort$PredKNNScoreV)
pred_valid = prepend(pred_valid,rep(0,n),before=1)
pred_valid = append(pred_valid,rep(0,n))
for(i in 1:VALIDSIZE){
  actural_p_valid[i] = sum(pred_valid[i:(i+2*n)])/(2*n+1)
}
valid_norm_sort = data.frame(valid_norm_sort,actural_p_valid)
png("Scatter plot diagram of KNN.png")
plot(valid_norm_sort$PredKNNScoreV,valid_norm_sort$actural_p_valid,xlab="Predicted Probability",ylab="Actual probability")
yy = valid_norm_sort$actural_p_valid
xx = valid_norm_sort$PredKNNScore
actual_fit = lm(yy~xx)
xx = seq(0,1:0.1)
yy = predict(actual_fit,data.frame((xx)))
lines(xx,yy)
summary(actual_fit)
legend(0.03,0.9,legend=c("y = 1.385x - 0.18","R^2 = 0.7089"))
dev.off()
```

Use the SSM(Sorting Smoothing Method) to estimate the real probability
```{r}
#1. order the valid data according to predictive probability
valid_norm_sort = valid_norm[order(PredScoreV),]
#2. use SSM formula to evaluate actural probability 'Pi', we choose n =50 according to the paper
VALIDSIZE = dim(valid_norm)[1]
n = 50
actural_p_valid = rep(0,VALIDSIZE)
#pred_valid = valid_sort$PredTreeScoreValid
pred_valid = valid_norm_sort$PredLabelV
pred_valid = prepend(pred_valid,rep(0,n),before=1)
pred_valid = append(pred_valid,rep(0,n))
for(i in 1:VALIDSIZE){
  actural_p_valid[i] = sum(pred_valid[i:(i+2*n)])/(2*n+1)
}
valid_norm_sort = data.frame(valid_norm_sort,actural_p_valid)
png("Scatter plot diagram of Logistic Regression.png")
plot(valid_norm_sort$PredScoreV,valid_norm_sort$actural_p_valid,xlab="Predicted Probability",ylab="Actual probability")
yy = valid_norm_sort$actural_p_valid
xx = valid_norm_sort$PredScore
actual_fit = lm(yy~xx)
xx = seq(0,1:0.1)
yy = predict(actual_fit,data.frame((xx)))
lines(xx,yy)
summary(actual_fit)
legend(0.03,0.9,legend=c("y = 1.375x - 0.23","R^2 = 0.6104"))
dev.off()
```